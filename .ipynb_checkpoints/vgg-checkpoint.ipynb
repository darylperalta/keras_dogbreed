{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "8a7bff8d-c95a-4cfb-8c38-14ab989f769b",
    "_uuid": "4da5a3e7db32799ca0108576e469157094d23111",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.csv\n",
      "sample_submission.csv\n",
      "test\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "im_size = 120\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "num_samples = 10222\n",
    "\n",
    "num_class = 120\n",
    "steps_per_epoch = num_samples//batch_size\n",
    "\n",
    "print(steps_per_epoch)\n",
    "epochs = 2\n",
    "\n",
    "def data_gen(batch_size, image_size):\n",
    "    \n",
    "    df_train = pd.read_csv('../input/labels.csv')\n",
    "    \n",
    "    targets_series = pd.Series(df_train['breed'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "\n",
    "\n",
    "    \n",
    "    fn = pd.Series(df_train['id'])\n",
    "\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "  \n",
    "   \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            #index = np.random.choice(fn.shape[0],1)\n",
    "            index = randint(0,fn.shape[0]-1)\n",
    "            img = cv2.imread('../input/train/{}.jpg'.format(fn[index]))\n",
    "            x_train.append(cv2.resize(img,(image_size,image_size)))\n",
    "            label = one_hot_labels[index]\n",
    "            y_train.append(label)\n",
    "        y_train_raw = np.array(y_train, np.uint8)\n",
    "        x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "        print(i)\n",
    "        yield x_train_raw, y_train_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "decf1fdf-5a24-41cd-b9b3-d49825efec82",
    "_uuid": "0946091bb67a488a05f9e39e8dac432b485eb70a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 120, 120, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 120, 120, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 120, 120, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 60, 60, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 30, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 15, 15, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               553080    \n",
      "=================================================================\n",
      "Total params: 20,577,464\n",
      "Trainable params: 553,080\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base pre-trained model\n",
    "# Can't download weights in the kernel\n",
    "base_model = VGG19(#weights='imagenet',\n",
    "    weights = 'imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "31\n",
      "31\n",
      "1/2 [==============>...............] - ETA: 2s - loss: 5.2660 - acc: 0.0000e+0031\n",
      "2/2 [==============================] - 3s - loss: 4.9182 - acc: 0.0174         \n",
      "Epoch 2/2\n",
      "31\n",
      "31\n",
      "31\n",
      "1/2 [==============>...............] - ETA: 1s - loss: 4.3264 - acc: 0.114631\n",
      "31\n",
      "2/2 [==============================] - 3s - loss: 4.2911 - acc: 0.1569     \n",
      "31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29df139cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_gen( batch_size=batch_size, image_size = im_size),\n",
    "                    steps_per_epoch=2,\n",
    "                    epochs=2, verbose =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c419742-1c6a-40c6-a26a-782836ee3641",
    "_uuid": "25ed4a3f9ce837128e3baa3e91bb0aee10e72ab8"
   },
   "source": [
    "Remember, accuracy is low here because we are not taking advantage of the pre-trained weights as they cannot be downloaded in the kernel. This means we are training the wights from scratch and I we have only run 1 epoch due to the hardware constraints in the kernel.\n",
    "\n",
    "Next we will make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "    \n",
    "\n",
    "df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('../input/test/{}.jpg'.format(f))\n",
    "    x_test.append(cv2.resize(img, (im_size, im_size)))\n",
    " \n",
    "x_test  = np.array(x_test, np.float32) / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f72763e1-68b6-4e54-9b80-ff0f4f94aa28",
    "_uuid": "e8a63ce4e324f3e011b3de7fc459301337d1b8d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0457a19d-0e26-4619-a3c8-78c71e7d41e6",
    "_uuid": "626b762d81d28ff77032658092d5bd9523b186b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(10358)\n",
    "df_test.to_csv('pred_vgg.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4ed0f81-8675-4a10-8b19-2d3ef191afe2",
    "_uuid": "b72af806cbbdea33edd7f73d24dbe9b9cbaae805",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
